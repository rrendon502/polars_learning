{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo 2: Lectura y Escritura de Archivos Parquet con Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Parquet es un formato de archivo columnar de código abierto diseñado para el almacenamiento eficiente de datos y el análisis de grandes volúmenes. Sus principales ventajas sobre formatos basados en filas como CSV son:\n",
    "- **Eficiencia de Almacenamiento:** Gracias a la compresión por columna y a los esquemas de codificación, los archivos Parquet suelen ser significativamente más pequeños que los CSV.\n",
    "- **Rendimiento de Consulta:** Las consultas que solo necesitan un subconjunto de columnas pueden leer solo esas columnas, evitando la I/O innecesaria de datos no requeridos.\n",
    "- **Preservación de Esquemas:** Parquet almacena el esquema de datos (tipos de datos de cada columna) dentro del propio archivo, lo que evita errores de inferencia de tipos al leerlos.\n",
    "\n",
    "Polars tiene un soporte de primera clase para Parquet, utilizando el motor Apache Arrow por debajo. Las funciones principales son `pl.read_parquet()` para leer y `DataFrame.write_parquet()` para escribir."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "# Crear un DataFrame de ejemplo (similar al usado en ejemplos de CSV)\n",
    "data = {\n",
    "    \"ID_Usuario\": [1, 2, 3, 4, 5],\n",
    "    \"Nombre\": [\"Alicia\", \"Roberto\", \"Carlos\", \"Diana\", \"Eduardo\"],\n",
    "    \"Email\": [\"a@example.com\", \"b@example.com\", \"c@example.com\", \"d@example.com\", \"e@example.com\"],\n",
    "    \"Edad\": [28, 34, 22, 45, None], # Valor nulo en Edad\n",
    "    \"Puntuacion\": [8.5, 9.1, 7.7, None, 6.9], # Valor nulo en Puntuacion\n",
    "    \"Fecha_Registro\": pl.date_range(pl.Date(2023, 1, 10), pl.Date(2023, 1, 14), \"1d\", eager=True),\n",
    "    \"Activo\": [True, False, True, False, True]\n",
    "}\n",
    "df_ejemplo = pl.DataFrame(data)\n",
    "\n",
    "print(\"DataFrame de Ejemplo:\")\n",
    "print(df_ejemplo)\n",
    "print(\"\\nGlimpse del DataFrame de Ejemplo:\")\n",
    "df_ejemplo.glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escribir a un Archivo Parquet (`DataFrame.write_parquet()`)\n",
    "\n",
    "Para guardar un DataFrame en formato Parquet, se utiliza el método `write_parquet()`. Simplemente se le proporciona la ruta del archivo.\n",
    "Polars (a través de Arrow) aplicará una compresión por defecto si está disponible (comúnmente `snappy`). Se puede especificar el algoritmo de compresión deseado mediante el parámetro `compression`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path_parquet_base = \"df_ejemplo.parquet\"\n",
    "file_path_parquet_gzip = \"df_ejemplo_gzip.parquet\"\n",
    "file_path_parquet_uncompressed = \"df_ejemplo_uncompressed.parquet\"\n",
    "\n",
    "# 1. Escritura básica\n",
    "# Polars usa 'zstd' por defecto si no se especifica y la feature está compilada, sino 'lz4', o 'uncompressed'.\n",
    "# Para consistencia, vamos a especificar 'uncompressed' para el base y luego comparar.\n",
    "df_ejemplo.write_parquet(file_path_parquet_uncompressed, compression='uncompressed')\n",
    "print(f\"DataFrame guardado en: {file_path_parquet_uncompressed} (sin compresión)\")\n",
    "\n",
    "# 2. Escritura especificando compresión 'snappy' (común y rápida)\n",
    "# Nota: La disponibilidad de algoritmos de compresión puede depender de cómo se compiló Polars/PyArrow.\n",
    "# 'snappy' es generalmente soportado.\n",
    "try:\n",
    "    df_ejemplo.write_parquet(file_path_parquet_base, compression='snappy')\n",
    "    print(f\"DataFrame guardado en: {file_path_parquet_base} (compresión snappy)\")\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo guardar con snappy: {e}. Guardando sin compresión en su lugar.\")\n",
    "    df_ejemplo.write_parquet(file_path_parquet_base, compression='uncompressed') # Fallback\n",
    "\n",
    "# 3. Escritura especificando compresión 'gzip' (buen ratio, más lento)\n",
    "df_ejemplo.write_parquet(file_path_parquet_gzip, compression='gzip')\n",
    "print(f\"DataFrame guardado en: {file_path_parquet_gzip} (compresión gzip)\")\n",
    "\n",
    "# 4. (Opcional) Comparar tamaños de archivo\n",
    "if os.path.exists(file_path_parquet_uncompressed):\n",
    "    size_uncompressed = os.path.getsize(file_path_parquet_uncompressed)\n",
    "    print(f\"\\nTamaño sin compresión: {size_uncompressed} bytes\")\n",
    "\n",
    "if os.path.exists(file_path_parquet_base):\n",
    "    size_base = os.path.getsize(file_path_parquet_base)\n",
    "    print(f\"Tamaño con compresión por defecto/snappy: {size_base} bytes\")\n",
    "\n",
    "if os.path.exists(file_path_parquet_gzip):\n",
    "    size_gzip = os.path.getsize(file_path_parquet_gzip)\n",
    "    print(f\"Tamaño con compresión gzip: {size_gzip} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer desde un Archivo Parquet (`pl.read_parquet()`)\n",
    "\n",
    "Para leer un archivo Parquet, se utiliza la función `pl.read_parquet()`. Una de las grandes ventajas es que Parquet almacena el esquema, por lo que Polars leerá los tipos de datos correctamente sin necesidad de inferencia compleja o especificación manual en la mayoría de los casos."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Usamos el archivo base (potencialmente con compresión snappy o sin compresión si snappy falló)\n",
    "df_leido_parquet = pl.read_parquet(file_path_parquet_base)\n",
    "\n",
    "print(\"\\nDataFrame leído desde Parquet (base):\")\n",
    "print(df_leido_parquet)\n",
    "\n",
    "print(\"\\nGlimpse del DataFrame leído:\")\n",
    "df_leido_parquet.glimpse()\n",
    "\n",
    "# Verificar igualdad\n",
    "# El método `equals()` es estricto. Compara tipos de datos y valores, incluyendo cómo se manejan los NaN.\n",
    "# Para floats, pequeñas diferencias de precisión podrían causar que `equals()` devuelva False.\n",
    "# Para nulos, `equals` los trata como iguales si ambos son nulos.\n",
    "print(f\"\\n¿Los DataFrames (original y leído) son iguales? {df_ejemplo.equals(df_leido_parquet)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de Columnas al Leer (`columns`)\n",
    "\n",
    "Una ventaja significativa de los formatos columnares como Parquet es la capacidad de leer solo las columnas necesarias. Esto puede ahorrar una cantidad considerable de tiempo y memoria, especialmente con DataFrames anchos (muchas columnas).\n",
    "El parámetro `columns` de `pl.read_parquet()` toma una lista de nombres de columna a leer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Seleccionar la primera ('ID_Usuario') y tercera ('Email') columna por nombre\n",
    "columnas_a_leer = [df_ejemplo.columns[0], df_ejemplo.columns[2]] \n",
    "\n",
    "df_leido_columnas = pl.read_parquet(file_path_parquet_base, columns=columnas_a_leer)\n",
    "\n",
    "print(f\"\\nDataFrame leído con columnas seleccionadas ({columnas_a_leer}):\")\n",
    "print(df_leido_columnas)\n",
    "df_leido_columnas.glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otras Opciones (Breve Mención)\n",
    "\n",
    "Tanto `read_parquet` como `write_parquet` ofrecen opciones adicionales para escenarios más avanzados:\n",
    "\n",
    "- **`n_rows` (para `read_parquet`):** Similar a `read_csv`, permite leer solo las primeras `N` filas del archivo Parquet. Útil para inspeccionar archivos grandes.\n",
    "- **`use_pyarrow` (para ambos):** Por defecto, Polars utiliza su propio lector/escritor de Parquet implementado en Rust, que es generalmente muy rápido. Si se establece en `True`, se utilizará la implementación de PyArrow. Esto podría ser útil para compatibilidad o si se necesitan características específicas de PyArrow.\n",
    "- **`pyarrow_options` (para ambos, si `use_pyarrow=True`):** Permite pasar un diccionario de opciones específicas para el motor PyArrow, como filtros de predicados (`filters`) durante la lectura para filtrar filas a nivel de I/O, o opciones de versión de Parquet al escribir.\n",
    "- **`row_group_index` / `row_group_predicate` (para `read_parquet`):** Parquet organiza los datos en \"row groups\". Estas opciones permiten leer selectivamente solo ciertos grupos de filas, lo que puede ser muy eficiente.\n",
    "- **`statistics=True` (para `write_parquet`):** Escribe metadatos estadísticos (min/max, conteo de nulos) para las columnas en el archivo Parquet. Esto puede ser usado por motores de consulta para optimizar aún más las lecturas (e.g., saltarse row groups que no cumplen un predicado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de Archivos Creados"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "archivos_a_eliminar = [\n",
    "    file_path_parquet_base,\n",
    "    file_path_parquet_gzip,\n",
    "    file_path_parquet_uncompressed\n",
    "]\n",
    "\n",
    "print(\"\\nLimpiando archivos generados...\")\n",
    "for f_path in archivos_a_eliminar:\n",
    "    if os.path.exists(f_path):\n",
    "        try:\n",
    "            os.remove(f_path)\n",
    "            print(f\"Archivo '{f_path}' eliminado.\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error al eliminar '{f_path}': {e.strerror}\")\n",
    "    else:\n",
    "        print(f\"Archivo '{f_path}' no encontrado (posiblemente ya eliminado o no creado).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
